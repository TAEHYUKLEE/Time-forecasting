{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Saved normalized data as csv format\"\n",
      "\n",
      "saved\\train1-49\n",
      "epoch:\"1\"\n",
      "Cost = 16.719659 Valdiation_error = 13.316581 L2_error = 10693.212891\n",
      "epoch:\"2\"\n",
      "Cost = 8.861279 Valdiation_error = 10.493930 L2_error = 10687.418945\n",
      "epoch:\"3\"\n",
      "Cost = 6.879084 Valdiation_error = 8.867489 L2_error = 10680.866211\n",
      "epoch:\"4\"\n",
      "Cost = 5.654895 Valdiation_error = 7.721517 L2_error = 10673.635742\n",
      "epoch:\"5\"\n",
      "Cost = 4.823987 Valdiation_error = 6.803364 L2_error = 10665.793945\n",
      "epoch:\"6\"\n",
      "Cost = 4.199654 Valdiation_error = 6.169612 L2_error = 10657.366211\n",
      "WARNING:tensorflow:From C:\\Users\\thlee\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "epoch:\"7\"\n",
      "Cost = 3.742926 Valdiation_error = 5.608327 L2_error = 10648.401367\n",
      "epoch:\"8\"\n",
      "Cost = 3.367874 Valdiation_error = 5.168886 L2_error = 10638.877930\n",
      "epoch:\"9\"\n",
      "Cost = 3.066588 Valdiation_error = 4.804605 L2_error = 10628.797852\n",
      "epoch:\"10\"\n",
      "Cost = 2.811416 Valdiation_error = 4.504317 L2_error = 10618.171875\n",
      "epoch:\"11\"\n",
      "Cost = 2.594599 Valdiation_error = 4.237133 L2_error = 10607.021484\n",
      "epoch:\"12\"\n",
      "Cost = 2.410376 Valdiation_error = 4.005937 L2_error = 10595.349609\n",
      "epoch:\"13\"\n",
      "Cost = 2.243426 Valdiation_error = 3.805382 L2_error = 10583.138672\n",
      "epoch:\"14\"\n",
      "Cost = 2.106077 Valdiation_error = 3.627953 L2_error = 10570.361328\n",
      "epoch:\"15\"\n",
      "Cost = 1.980203 Valdiation_error = 3.467602 L2_error = 10557.060547\n",
      "epoch:\"16\"\n",
      "Cost = 1.873120 Valdiation_error = 3.328070 L2_error = 10543.195312\n",
      "epoch:\"17\"\n",
      "Cost = 1.771115 Valdiation_error = 3.190246 L2_error = 10528.761719\n",
      "epoch:\"18\"\n",
      "Cost = 1.680881 Valdiation_error = 3.086354 L2_error = 10513.759766\n",
      "epoch:\"19\"\n",
      "Cost = 1.605119 Valdiation_error = 2.972307 L2_error = 10498.169922\n",
      "epoch:\"20\"\n",
      "Cost = 1.532589 Valdiation_error = 2.867550 L2_error = 10481.978516\n",
      "epoch:\"21\"\n",
      "Cost = 1.468561 Valdiation_error = 2.783943 L2_error = 10465.179688\n",
      "epoch:\"22\"\n",
      "Cost = 1.405959 Valdiation_error = 2.696649 L2_error = 10447.753906\n",
      "epoch:\"23\"\n",
      "Cost = 1.353936 Valdiation_error = 2.623647 L2_error = 10429.695312\n",
      "epoch:\"24\"\n",
      "Cost = 1.303839 Valdiation_error = 2.556063 L2_error = 10410.978516\n",
      "epoch:\"25\"\n",
      "Cost = 1.258162 Valdiation_error = 2.496394 L2_error = 10391.583008\n",
      "epoch:\"26\"\n",
      "Cost = 1.216060 Valdiation_error = 2.434826 L2_error = 10371.499023\n",
      "epoch:\"27\"\n",
      "Cost = 1.181436 Valdiation_error = 2.376013 L2_error = 10350.729492\n",
      "epoch:\"28\"\n",
      "Cost = 1.143309 Valdiation_error = 2.330032 L2_error = 10329.248047\n",
      "epoch:\"29\"\n",
      "Cost = 1.111532 Valdiation_error = 2.279365 L2_error = 10307.072266\n",
      "epoch:\"30\"\n",
      "Cost = 1.083956 Valdiation_error = 2.247743 L2_error = 10284.139648\n",
      "epoch:\"31\"\n",
      "Cost = 1.053277 Valdiation_error = 2.190367 L2_error = 10260.493164\n",
      "epoch:\"32\"\n",
      "Cost = 1.027755 Valdiation_error = 2.154605 L2_error = 10236.059570\n",
      "epoch:\"33\"\n",
      "Cost = 1.003326 Valdiation_error = 2.123632 L2_error = 10210.884766\n",
      "epoch:\"34\"\n",
      "Cost = 0.980076 Valdiation_error = 2.072030 L2_error = 10184.930664\n",
      "epoch:\"35\"\n",
      "Cost = 0.957512 Valdiation_error = 2.046184 L2_error = 10158.234375\n",
      "epoch:\"36\"\n",
      "Cost = 0.936161 Valdiation_error = 2.029564 L2_error = 10130.752930\n",
      "epoch:\"37\"\n",
      "Cost = 0.915267 Valdiation_error = 1.996834 L2_error = 10102.488281\n",
      "epoch:\"38\"\n",
      "Cost = 0.896659 Valdiation_error = 1.968671 L2_error = 10073.456055\n",
      "epoch:\"39\"\n",
      "Cost = 0.877577 Valdiation_error = 1.970458 L2_error = 10043.671875\n",
      "epoch:\"40\"\n",
      "Cost = 0.860266 Valdiation_error = 1.930358 L2_error = 10013.099609\n",
      "epoch:\"41\"\n",
      "Cost = 0.843976 Valdiation_error = 1.905332 L2_error = 9981.732422\n",
      "epoch:\"42\"\n",
      "Cost = 0.827340 Valdiation_error = 1.892422 L2_error = 9949.614258\n",
      "epoch:\"43\"\n",
      "Cost = 0.811368 Valdiation_error = 1.865471 L2_error = 9916.703125\n",
      "epoch:\"44\"\n",
      "Cost = 0.796777 Valdiation_error = 1.858348 L2_error = 9883.064453\n",
      "epoch:\"45\"\n",
      "Cost = 0.783272 Valdiation_error = 1.826434 L2_error = 9848.673828\n",
      "epoch:\"46\"\n",
      "Cost = 0.769654 Valdiation_error = 1.793474 L2_error = 9813.542969\n",
      "epoch:\"47\"\n",
      "Cost = 0.756275 Valdiation_error = 1.784606 L2_error = 9777.697266\n",
      "epoch:\"48\"\n",
      "Cost = 0.743206 Valdiation_error = 1.765414 L2_error = 9741.122070\n",
      "epoch:\"49\"\n",
      "Cost = 0.731408 Valdiation_error = 1.772744 L2_error = 9703.900391\n",
      "epoch:\"50\"\n",
      "Cost = 0.718561 Valdiation_error = 1.739855 L2_error = 9665.976562\n",
      "Finished!!!\n",
      "\n",
      "\n",
      "Let's start testing the learned model\n",
      "Accuracy error = 0.775689\n"
     ]
    }
   ],
   "source": [
    "#용상 Project only NN code\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd # to read the dataset as dataframe\n",
    "import random #for the random.shuffle function\n",
    "import tensorflow.compat.v1 as tf; tf.compat.v1.disable_eager_execution()\n",
    "import normalization as norm # 직접 만든 normalization module\n",
    "import csv \n",
    "\n",
    "df= pd.read_csv(\"C:\\Read\\Final dataset.csv\", encoding = \"ISO-8859-1\") \n",
    "\n",
    "f = open('data file.csv', 'w', encoding='utf-8', newline='')\n",
    "\n",
    "(norm, R, C) = norm.norm(df) #normalization module 만들어놓음\n",
    "\n",
    "######### Split full dataset into training(70%), validation(20%), test(10%) ########\n",
    "N_tr = R*7//10\n",
    "N_va = R*2//10\n",
    "N_te = R - (N_tr+ N_va)\n",
    "\n",
    "\n",
    "train_fr = norm.iloc[0:N_tr, :]\n",
    "val_fr = norm.iloc[N_tr:(N_va+N_tr), :]\n",
    "test_fr = norm.iloc[(N_va+N_tr):(N_va+N_tr+N_te), :]\n",
    "\n",
    "\n",
    "train_set = np.array(train_fr)\n",
    "val_set = np.array(val_fr)\n",
    "test_set = np.array(test_fr)\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################    formation of Neural Netowrk  ################################# \n",
    "\n",
    "#Neural Network size parameter\n",
    "n_input = (C-1) #C is numbers of variables\n",
    "n_output =1\n",
    "node_layer_1 = 128\n",
    "node_layer_2 = 128\n",
    "node_layer_3 = 128\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "#tensor들에 대한 Input output, variable들을 정의해준다 (define tensor)\n",
    "X_input = tf.placeholder(\"float\", [None, n_input]) #type tensor type (데이터 유형)\n",
    "Y_real = tf.placeholder(\"float\", [None, n_output]) #나중에 Loss에서 쓰일 y값\n",
    "\n",
    "weight_1 = tf.Variable(tf.random_normal([n_input, node_layer_1]))\n",
    "weight_2 = tf.Variable(tf.random_normal([node_layer_1, node_layer_2]))\n",
    "weight_3 = tf.Variable(tf.random_normal([node_layer_2, node_layer_3]))\n",
    "weight_out = tf.Variable(tf.random_normal([node_layer_3, n_output]))\n",
    "\n",
    "bias_1 = tf.Variable(tf.random_normal([1, node_layer_1]), dtype=tf.float32)\n",
    "bias_2 = tf.Variable(tf.random_normal([1, node_layer_2]), dtype=tf.float32)\n",
    "bias_3 = tf.Variable(tf.random_normal([1, node_layer_3]), dtype=tf.float32)\n",
    "bias_out = tf.Variable(tf.random_normal([n_output]))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "#shape은 numpy, tensor와 혼용해서 쓰지 못하네\n",
    "\n",
    "# Formation of Neural Network \n",
    "def MLP (X_input):\n",
    "    \n",
    "    #모든 w_1 ~ w_N (Hidden layer) w_1 ~ w_N' (Hidden layer) 모든 경우의수의 조합을 Matrix로 나타내게 된 것\n",
    "    linear_1 = tf.matmul(X_input, weight_1) + bias_1\n",
    "    active_1 = tf.nn.sigmoid(linear_1) #sigmoid를 하든 tanh하든, relu를 하든 activation function은 알아서 정하기를\n",
    "    #active matrix dimension [batch_size, node number_layer1]\n",
    "    #weight_2 matrix dimension [node number_layer1, node_layer_2]\n",
    "    \n",
    "    linear_2 = tf.add(tf.matmul(active_1, weight_2), bias_2) #tf.add로 안하고 그냥 더하면 data type문제가 생김\n",
    "    active_2 = tf.nn.sigmoid(linear_2)\n",
    "    #Numpy는 Dataframe말고 array나 list 뿐만 아니라 Tensor까지 모두 빠르게 계산해준다.\n",
    "    \n",
    "    linear_3 = tf.matmul(active_2, weight_3) + bias_3\n",
    "    active_3 = tf.nn.sigmoid(linear_3)\n",
    "    \n",
    "    Y_out = tf.matmul(active_3, weight_out) + bias_out \n",
    "\n",
    "    return Y_out\n",
    "\n",
    "########### Define Loss function - learning part ##############\n",
    "#learning parameter\n",
    "learning_rate = 0.0001\n",
    "B = 0.001\n",
    "\n",
    "Y_p =  MLP(X_input)\n",
    "\n",
    "MSE = tf.reduce_mean((Y_p - Y_real)**2)\n",
    "\n",
    "#L2 regularization\n",
    "regular = tf.nn.l2_loss(weight_1)+tf.nn.l2_loss(weight_2)+tf.nn.l2_loss(weight_out)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "Loss = MSE + B*regular\n",
    "\n",
    "train = optimizer.minimize(Loss)  #앞에서 구한 Loss에 대해 AdamOptimizer로 train (Optimization) 시킨다\n",
    "\n",
    "###################################################################\n",
    "\n",
    "sess =  tf.Session() \n",
    "sess.run(tf.global_variables_initializer()) \n",
    "#모든 tensor의 variable들을 initialization할 수 있다. (딱 초기화까지 시켜놓고 끝냄)\n",
    "\n",
    "# 위에서 Neural network에 필요한 size와 구성을 끝냈다. 불러온 데이터를 통해 Batch를 형성하고 넣어줘야한다\n",
    "\n",
    "\n",
    "# Batch는 training 목적에 따라 형성 시키는것이 조금씩 달라진다.\n",
    "\n",
    "\n",
    "#Batch Parameter\n",
    "N_batch = train_set.shape[0]//batch_size\n",
    "# 여기서 있는 그대로 학습을 시키기 위해 Random하게 Batchsize 100개로 Training epoch 개수 (전체 Instnace 개수 //100 개를 한다)\n",
    "\n",
    "# Training batch\n",
    "batch_X = np.zeros((batch_size,C-1)) #Batch 형태를 잡아줌\n",
    "batch_Y = np.zeros((batch_size,1))\n",
    "\n",
    "# Validation batch\n",
    "batch_val_X = np.zeros((val_set.shape[0],C-1)) #Batch 형태를 잡아줌\n",
    "batch_val_Y = np.zeros((val_set.shape[0],1))\n",
    "\n",
    "# Test batch\n",
    "batch_test_X = np.zeros((test_set.shape[0],C-1)) #Batch 형태를 잡아줌\n",
    "batch_test_Y = np.zeros((test_set.shape[0],1))\n",
    "\n",
    "#print(batch_X.shape)\n",
    "#print(train_set.shape)\n",
    "\n",
    "\n",
    "def formation_batch(batch_X, batch_Y, train_set, index, batch_size):\n",
    "    for i in range(batch_size):\n",
    "       # print(index.shape)\n",
    "        batch_X[i:i+1,:] = train_set[index[i]:index[i]+1,1:C]\n",
    "        #index[i]:index[i+1] -- 이건 왜 안될까? index[i]:index[i]+1로 해야하네\n",
    "        #ValueError: could not broadcast input array from shape (0,38) into shape (1,38)\n",
    "        batch_Y[i:i+1,:] = train_set[index[i]:index[i]+1,0:1] #여기서 위에서 나눠 놓은 Train set을 이용한다\n",
    "    return batch_X, batch_Y    \n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "path = tf.train.latest_checkpoint(\"saved\")\n",
    "print(path)\n",
    "\n",
    "\n",
    "    \n",
    "# Training part    \n",
    "epoch_num = 50\n",
    "\n",
    "#Shuffle 시킬 index\n",
    "index=np.arange(train_set.shape[0]) \n",
    "#index에서 batch_size를 빼야하지 않나? 왜 안빼니까 되는거지?\n",
    "#한 Instance 가 하나의 batch 요소로 들어가기때문에 train_set row number에서 무언가 따로 뺄 필요가 없다\n",
    "#착각함... Batch자체는 여러 행에서 무작위로 뽑아다 형성시키는거였음\n",
    "#print(index)\n",
    "#print(N_batch)\n",
    "\n",
    "#만약 index가 R이면 그 이상의 index가 없다\n",
    "\n",
    "\n",
    "f.write('\"epoch\", \"Cost\", \"Val_error\", \"L2_error\" \\n')\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    cost_mean = 0\n",
    "    random.shuffle(index)  \n",
    "       \n",
    "    for i in range(N_batch):\n",
    "                        \n",
    "                                     ############# Training part ##############\n",
    "       #Batch를 만들어서 가져온다\n",
    "        batch_X, batch_Y = formation_batch(batch_X, batch_Y, train_set, index[(i)*batch_size:(i+1)*batch_size], batch_size)        \n",
    "                 \n",
    "        #학습시키는 구문\n",
    "        #batch_X에서는 Y_pred을 뽑아내고, Y_real에서는 진짜 Y_real값을 가져온다\n",
    "        # sess.run(train, feed_dict={X_input: batch_X,Y_real: batch_Y})\n",
    "        #B = sess.run(MSE, feed_dict={X_input: batch_X,Y_real: batch_Y})\n",
    "        \n",
    "        _, cost = sess.run([train, MSE], feed_dict={X_input: batch_X,Y_real: batch_Y})\n",
    "        # Compute average loss\n",
    "        cost_mean += cost /N_batch   \n",
    "    \n",
    "    #위에서 하나의 Batch로 학습을 시킨 [weight, bias]를 가지고 Validation을 해 보자.  \n",
    "\n",
    "    \n",
    "                                     ############# Validation part ################\n",
    "        \n",
    "    #위에서 한나의 Batch로 학습시킨 것에 대해서 Validation해보자\n",
    "    Val_error =0\n",
    "    index_val = np.arange(val_set.shape[0]) \n",
    "    N_val = len(index_val) #index_val 개수를 채워줘야한다\n",
    "    #print(N_val)\n",
    "    #Batch를 채워와야 한다. Validation batch는 training batch와 size가 다르니까 새로 선언해줘야한다\n",
    "\n",
    "    batch_val_X, batch_val_Y = formation_batch(batch_val_X, batch_val_Y, val_set, index_val, N_val)\n",
    "    ##### Validation set data하고 빈 batch_val_X하고 Index하고 같이 보내줘서 validation batch를 만들어 온다\n",
    "\n",
    "    Y_val_op = MLP(X_input)\n",
    "    #앞에 만들어놓은 MLP를 그대로 이용 바로 위에서 만들어온 VALIDATION Batch를 넣을 Neural network를 다시 만들어준다\n",
    "    \n",
    "\n",
    "    val_pre, val_real = sess.run([Y_val_op,Y_real], feed_dict={X_input: batch_val_X, Y_real: batch_val_Y})\n",
    "    #Feed로 앞에서 만들어온 batch_val을 넣어서 feeding해준다 Y_val_op (MLP) 와 Y_real에 넣어준다\n",
    "\n",
    "    \n",
    "    #Validation error를 계산해낸다.\n",
    "\n",
    "    Val_error = np.mean((val_pre[:] - val_real[:])**2)\n",
    "    #여기서 tf.reduce_mean을 쓰면 편하지만, sess.run()을 한 번 더 해줘야 하니까 #np.mean\n",
    "    \n",
    "    # Display logs per epoch step\n",
    "    if epoch % 1 == 0: #epoch 단계마다 보겠다 1마다 보겠다라는 것을 의미함.\n",
    "        L2_error = sess.run(regular) #loss_op2 beta* regularizer 하위 node부터 계산해서 위의 계산값을 보겠다.\n",
    "        \n",
    "        print('epoch:\"%d\"' % (epoch+1))\n",
    "        print(\"Cost = %f\" %cost_mean, \"Valdiation_error = %f\" %Val_error,\"L2_error = %f\" %L2_error)\n",
    "        \n",
    "    ### Write Cost & Validation ###\n",
    "    f.write(\"%d, %f, %f, %f \\n\" %(epoch+1, cost_mean, Val_error, L2_error))\n",
    "    \n",
    "    ckpt_path = saver.save(sess, \"save_file/train1\", epoch)\n",
    "        \n",
    "f.close()    \n",
    "\n",
    "print(\"Finished!!!\\n\")\n",
    "print(\"\\nLet's start testing the learned model\")\n",
    "    \n",
    "                                ################### Test part #####################\n",
    "test_error =0\n",
    "index_test = np.arange(test_set.shape[0])\n",
    "N_test = len(index_test) #index_val 개수를 채워줘야한다\n",
    "\n",
    "batch_test_X, batch_test_Y = formation_batch(batch_test_X, batch_test_Y, test_set, index_val, N_test)\n",
    "##### test set data하고 빈 batch_test_X하고 Index하고 같이 보내줘서 test batch를 만들어 온다\n",
    "\n",
    "Y_test_op = MLP(X_input)\n",
    "#앞에 만들어놓은 MLP를 그대로 이용 바로 위에서 만들어온 test Batch를 넣을 Neural network를 다시 만들어준다   \n",
    "\n",
    "test_pre, test_real = sess.run([Y_test_op,Y_real], feed_dict={X_input: batch_test_X, Y_real: batch_test_Y})\n",
    "#Feed로 앞에서 만들어온 batch_test을 넣어서 feeding해준다 Y_test_op (MLP) 와 Y_real에 넣어준다\n",
    "    \n",
    "#test error를 계산한다\n",
    "\n",
    "test_error = np.mean((test_pre[:] - test_real[:])**2)\n",
    "#여기서 tf.reduce_mean을 쓰면 편하지만, sess.run()을 한 번 더 해줘야 하니까 #np.mean\n",
    "\n",
    "print(\"Accuracy error = %f\" %test_error)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
